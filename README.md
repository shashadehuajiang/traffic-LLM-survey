# traffic-LLM-survey
some related work about traffic and large language model 





## ref (useful)
1. [Transformer 是 RNN：具有线性注意力的快速自回归 Transformer](https://proceedings.mlr.press/v119/katharopoulos20a.html)

内容很好，略。

2. [A survey of transformers](https://www.sciencedirect.com/science/article/pii/S2666651022000146)

较全的一个survey

3. [On The Computational Complexity of Self-Attention](https://proceedings.mlr.press/v201/duman-keles23a.html)

结论：在强指数时间假设下，在最坏情况下，这里可能存在一种基本的“没有免费午餐”现象：似乎不太可能获得自注意力的可证明的次二次算法，同时对于所有输入也是可证明的近似准确的算法。

4. [Training Vision Transformers with Only 2040 Images](https://link.springer.com/chapter/10.1007/978-3-031-19806-9_13)



## Efficient Transformers

1. [Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model](https://arxiv.org/abs/2305.16340)

2. [Block-State Transformers](https://arxiv.org/pdf/2306.09539.pdf)

3. [扩散器：具有长序列多跳注意力扩散的高效变压器](https://ojs.aaai.org/index.php/AAAI/article/view/26502)

## other ref

1. [A review on the attention mechanism of deep learning](https://www.sciencedirect.com/science/article/pii/S092523122100477X)
