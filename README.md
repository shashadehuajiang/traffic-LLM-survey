# traffic-LLM-survey
some related work about traffic and large language model 





## ref
1. [Transformer 是 RNN：具有线性注意力的快速自回归 Transformer](https://proceedings.mlr.press/v119/katharopoulos20a.html)

内容很好，略。

2. [A survey of transformers](https://www.sciencedirect.com/science/article/pii/S2666651022000146)

较全的一个survey

3. [On The Computational Complexity of Self-Attention](https://proceedings.mlr.press/v201/duman-keles23a.html)

结论：在强指数时间假设下，在最坏情况下，这里可能存在一种基本的“没有免费午餐”现象：似乎不太可能获得自注意力的可证明的次二次算法，同时对于所有输入也是可证明的近似准确的算法。

4. 

5. [扩散器：具有长序列多跳注意力扩散的高效变压器]()

## Efficient Transformers

1. []




